{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "peripheral-labor",
   "metadata": {},
   "outputs": [],
   "source": [
    "from HelperFunctions import *\n",
    "\n",
    "n = 100\n",
    "p = 5 \n",
    "grid_dim = 5 \n",
    "sigma = [0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "noise = 0.25\n",
    "degree = 3\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "applied-rally",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged after 715 steps\n",
      "Direct Solution SPO loss: 7.082644288733791\n",
      "Stochastic Gradient Descent Solution SPO loss: 9.308088975014144\n",
      "Direct Solution SPO plus loss: 24.58340650424476\n",
      "Stochastic Gradient Descent Solution SPO plus loss: 33.960153768394\n"
     ]
    }
   ],
   "source": [
    "X, C = generate_data(n, p, grid_dim, sigma, noise, degree)\n",
    "A,b = CreateShortestPathConstraints(grid_dim)\n",
<<<<<<< HEAD
    "B1 = DirectSolution(A, b, X, C)"
=======
    "B_direct=DirectSolution(A,b, X, C)\n",
    "B_SGD=GradientDescentSolution(A,b, X, C, batch_size=10,epsilon = 0.001) \n",
    "\n",
    "solver = ShortestPathSolver(A,b)\n",
    "loss_direct = SPOLoss(solver, X, C, B_direct)\n",
    "loss_SGD = SPOLoss(solver, X, C, B_SGD)\n",
    "print(f\"Direct Solution SPO loss: {loss_direct}\")\n",
    "print(f\"Stochastic Gradient Descent Solution SPO loss: {loss_SGD}\")\n",
    "\n",
    "\n",
    "loss_plus_direct = SPOplusLoss(solver, X, C, B_direct)\n",
    "loss_plus_SGD = SPOplusLoss(solver, X, C, B_SGD)\n",
    "print(f\"Direct Solution SPO plus loss: {loss_plus_direct}\")\n",
    "print(f\"Stochastic Gradient Descent Solution SPO plus loss: {loss_plus_SGD}\")\n",
    "\n"
>>>>>>> 180c3a30d2471a9f71164389427bdefe87dda679
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "enormous-unknown",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Converged after 666 steps\n",
      "Direct Solution SPO loss: 7.2181806011614755\n",
      "Stochastic Gradient Descent Solution SPO loss: 9.900696443773086\n",
      "Direct Solution SPO plus loss: 24.15557983570721\n",
      "Stochastic Gradient Descent Solution SPO plus loss: 32.65655551271321\n",
      "1\n",
      "Converged after 498 steps\n",
      "Direct Solution SPO loss: 7.275274866163491\n",
      "Stochastic Gradient Descent Solution SPO loss: 8.9639018524305\n",
      "Direct Solution SPO plus loss: 23.627487471333726\n",
      "Stochastic Gradient Descent Solution SPO plus loss: 35.88750331690197\n",
      "2\n",
      "Converged after 381 steps\n",
      "Direct Solution SPO loss: 8.44327077514372\n",
      "Stochastic Gradient Descent Solution SPO loss: 10.081569621882627\n",
      "Direct Solution SPO plus loss: 27.073637540375458\n",
      "Stochastic Gradient Descent Solution SPO plus loss: 36.59686238389652\n",
      "3\n",
      "Converged after 606 steps\n",
      "Direct Solution SPO loss: 6.543396503439653\n",
      "Stochastic Gradient Descent Solution SPO loss: 7.113887729812632\n",
      "Direct Solution SPO plus loss: 22.792856405938522\n",
      "Stochastic Gradient Descent Solution SPO plus loss: 31.65539617088593\n",
      "4\n",
      "Converged after 593 steps\n",
      "Direct Solution SPO loss: 6.962279471678776\n",
      "Stochastic Gradient Descent Solution SPO loss: 10.05412453474186\n",
      "Direct Solution SPO plus loss: 24.43185188978235\n",
      "Stochastic Gradient Descent Solution SPO plus loss: 32.26105188786543\n",
      "5\n",
      "Converged after 444 steps\n",
      "Direct Solution SPO loss: 9.480043539894831\n",
      "Stochastic Gradient Descent Solution SPO loss: 10.822991398602898\n",
      "Direct Solution SPO plus loss: 25.738683072786053\n",
      "Stochastic Gradient Descent Solution SPO plus loss: 37.1076951425948\n",
      "6\n",
      "Converged after 510 steps\n",
      "Direct Solution SPO loss: 8.022577826010238\n",
      "Stochastic Gradient Descent Solution SPO loss: 9.20612959573853\n",
      "Direct Solution SPO plus loss: 25.67192847303137\n",
      "Stochastic Gradient Descent Solution SPO plus loss: 34.81471329862015\n",
      "7\n",
      "Converged after 667 steps\n",
      "Direct Solution SPO loss: 7.2620237023940355\n",
      "Stochastic Gradient Descent Solution SPO loss: 10.114653801721252\n",
      "Direct Solution SPO plus loss: 24.247422526942056\n",
      "Stochastic Gradient Descent Solution SPO plus loss: 34.223332180498474\n",
      "8\n",
      "Converged after 538 steps\n",
      "Direct Solution SPO loss: 8.337307436881092\n",
      "Stochastic Gradient Descent Solution SPO loss: 10.34609009613905\n",
      "Direct Solution SPO plus loss: 27.022191050944922\n",
      "Stochastic Gradient Descent Solution SPO plus loss: 36.6053264111509\n",
      "9\n",
      "Converged after 429 steps\n",
      "Direct Solution SPO loss: 8.63016065700591\n",
      "Stochastic Gradient Descent Solution SPO loss: 9.47528640208926\n",
      "Direct Solution SPO plus loss: 26.673562733137143\n",
      "Stochastic Gradient Descent Solution SPO plus loss: 36.58975845371887\n",
      "10\n",
      "Converged after 665 steps\n",
      "Direct Solution SPO loss: 7.540351872088285\n",
      "Stochastic Gradient Descent Solution SPO loss: 8.761793945215395\n",
      "Direct Solution SPO plus loss: 25.2291857930169\n",
      "Stochastic Gradient Descent Solution SPO plus loss: 32.97308736882944\n",
      "11\n",
      "Converged after 594 steps\n",
      "Direct Solution SPO loss: 7.510613501269872\n",
      "Stochastic Gradient Descent Solution SPO loss: 7.795309685192515\n",
      "Direct Solution SPO plus loss: 25.226828327252573\n",
      "Stochastic Gradient Descent Solution SPO plus loss: 36.95099162126697\n",
      "12\n",
      "Converged after 546 steps\n",
      "Direct Solution SPO loss: 8.203276538864193\n",
      "Stochastic Gradient Descent Solution SPO loss: 10.156823071196378\n",
      "Direct Solution SPO plus loss: 24.807171007299807\n",
      "Stochastic Gradient Descent Solution SPO plus loss: 35.592206508422166\n",
      "13\n",
      "Converged after 506 steps\n",
      "Direct Solution SPO loss: 7.0010844139329125\n",
      "Stochastic Gradient Descent Solution SPO loss: 9.03372742707259\n",
      "Direct Solution SPO plus loss: 24.17192888055351\n",
      "Stochastic Gradient Descent Solution SPO plus loss: 35.458043446308636\n",
      "14\n",
      "Converged after 395 steps\n",
      "Direct Solution SPO loss: 9.074291624218386\n",
      "Stochastic Gradient Descent Solution SPO loss: 9.476613416826003\n",
      "Direct Solution SPO plus loss: 26.795653875275743\n",
      "Stochastic Gradient Descent Solution SPO plus loss: 36.08128950676199\n",
      "15\n",
      "Converged after 730 steps\n",
      "Direct Solution SPO loss: 7.703378203161119\n",
      "Stochastic Gradient Descent Solution SPO loss: 9.956508199468372\n",
      "Direct Solution SPO plus loss: 22.3312466705042\n",
      "Stochastic Gradient Descent Solution SPO plus loss: 30.74526656970141\n",
      "16\n",
      "Converged after 367 steps\n",
      "Direct Solution SPO loss: 8.161350245849171\n",
      "Stochastic Gradient Descent Solution SPO loss: 9.850518521978048\n",
      "Direct Solution SPO plus loss: 23.82424588880896\n",
      "Stochastic Gradient Descent Solution SPO plus loss: 35.75180899023521\n",
      "17\n",
      "Converged after 585 steps\n",
      "Direct Solution SPO loss: 8.293499900239695\n",
      "Stochastic Gradient Descent Solution SPO loss: 8.969239379079191\n",
      "Direct Solution SPO plus loss: 24.841922986014648\n",
      "Stochastic Gradient Descent Solution SPO plus loss: 34.680848201069246\n",
      "18\n",
      "Converged after 523 steps\n",
      "Direct Solution SPO loss: 6.6282314293576565\n",
      "Stochastic Gradient Descent Solution SPO loss: 8.572587377135205\n",
      "Direct Solution SPO plus loss: 25.44742448069945\n",
      "Stochastic Gradient Descent Solution SPO plus loss: 36.595559547893366\n",
      "19\n",
      "Converged after 570 steps\n",
      "Direct Solution SPO loss: 8.110300226632608\n",
      "Stochastic Gradient Descent Solution SPO loss: 9.896863499731916\n",
      "Direct Solution SPO plus loss: 25.905372953000313\n",
      "Stochastic Gradient Descent Solution SPO plus loss: 34.08186134157802\n",
      "20\n",
      "Converged after 569 steps\n",
      "Direct Solution SPO loss: 9.053813586695323\n",
      "Stochastic Gradient Descent Solution SPO loss: 10.409014304031661\n",
      "Direct Solution SPO plus loss: 30.410101170233165\n",
      "Stochastic Gradient Descent Solution SPO plus loss: 36.997505293128995\n",
      "21\n",
      "Converged after 505 steps\n",
      "Direct Solution SPO loss: 7.4553218415183435\n",
      "Stochastic Gradient Descent Solution SPO loss: 9.355186861438051\n",
      "Direct Solution SPO plus loss: 24.06619478603531\n",
      "Stochastic Gradient Descent Solution SPO plus loss: 33.988220781006696\n",
      "22\n",
      "Converged after 676 steps\n",
      "Direct Solution SPO loss: 8.337856151187953\n",
      "Stochastic Gradient Descent Solution SPO loss: 10.096318193426196\n",
      "Direct Solution SPO plus loss: 27.43503066588347\n",
      "Stochastic Gradient Descent Solution SPO plus loss: 35.26341752417391\n",
      "23\n",
      "Converged after 634 steps\n",
      "Direct Solution SPO loss: 8.905259454830084\n",
      "Stochastic Gradient Descent Solution SPO loss: 10.46470461940724\n",
      "Direct Solution SPO plus loss: 26.268870950381334\n",
      "Stochastic Gradient Descent Solution SPO plus loss: 35.23057882933555\n",
      "24\n",
      "Converged after 572 steps\n",
      "Direct Solution SPO loss: 6.526350658097399\n",
      "Stochastic Gradient Descent Solution SPO loss: 8.454330768326582\n",
      "Direct Solution SPO plus loss: 22.960905115869956\n",
      "Stochastic Gradient Descent Solution SPO plus loss: 31.914530160216078\n",
      "25\n",
      "Converged after 699 steps\n",
      "Direct Solution SPO loss: 7.437170645006481\n",
      "Stochastic Gradient Descent Solution SPO loss: 9.248028530263836\n",
      "Direct Solution SPO plus loss: 24.39489195674415\n",
      "Stochastic Gradient Descent Solution SPO plus loss: 33.60759995598299\n",
      "26\n",
      "Converged after 490 steps\n",
      "Direct Solution SPO loss: 8.759121013358017\n",
      "Stochastic Gradient Descent Solution SPO loss: 10.443290899811108\n",
      "Direct Solution SPO plus loss: 25.98094936331609\n",
      "Stochastic Gradient Descent Solution SPO plus loss: 34.693171894763644\n",
      "27\n",
      "Converged after 485 steps\n",
      "Direct Solution SPO loss: 6.578747591979955\n",
      "Stochastic Gradient Descent Solution SPO loss: 10.596509087743252\n",
      "Direct Solution SPO plus loss: 25.123372197097865\n",
      "Stochastic Gradient Descent Solution SPO plus loss: 35.79432361092538\n",
      "28\n",
      "Converged after 446 steps\n",
      "Direct Solution SPO loss: 8.973927309633872\n",
      "Stochastic Gradient Descent Solution SPO loss: 10.96251122153375\n",
      "Direct Solution SPO plus loss: 28.023933850854725\n",
      "Stochastic Gradient Descent Solution SPO plus loss: 37.958008472939696\n",
      "29\n",
      "Converged after 548 steps\n",
      "Direct Solution SPO loss: 6.51544169561792\n",
      "Stochastic Gradient Descent Solution SPO loss: 7.560806848566972\n",
      "Direct Solution SPO plus loss: 23.515696147640494\n",
      "Stochastic Gradient Descent Solution SPO plus loss: 34.292666866660795\n",
      "30\n",
      "Converged after 529 steps\n",
      "Direct Solution SPO loss: 8.414827084593634\n",
      "Stochastic Gradient Descent Solution SPO loss: 11.129836912262835\n",
      "Direct Solution SPO plus loss: 25.60708279102198\n",
      "Stochastic Gradient Descent Solution SPO plus loss: 37.85579283414882\n",
      "31\n",
      "Converged after 533 steps\n",
      "Direct Solution SPO loss: 7.308176332352523\n",
      "Stochastic Gradient Descent Solution SPO loss: 9.41418424106454\n",
      "Direct Solution SPO plus loss: 24.528535671481066\n",
      "Stochastic Gradient Descent Solution SPO plus loss: 37.039063851251484\n",
      "32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged after 424 steps\n",
      "Direct Solution SPO loss: 5.660244816746163\n",
      "Stochastic Gradient Descent Solution SPO loss: 9.635404397916828\n",
      "Direct Solution SPO plus loss: 22.089403152414697\n",
      "Stochastic Gradient Descent Solution SPO plus loss: 36.22109334025607\n",
      "33\n",
      "Converged after 724 steps\n",
      "Direct Solution SPO loss: 7.660574988981207\n",
      "Stochastic Gradient Descent Solution SPO loss: 8.587230267495652\n",
      "Direct Solution SPO plus loss: 25.452717341500044\n",
      "Stochastic Gradient Descent Solution SPO plus loss: 34.37086210292777\n",
      "34\n",
      "Converged after 494 steps\n",
      "Direct Solution SPO loss: 7.348726026904587\n",
      "Stochastic Gradient Descent Solution SPO loss: 9.669396381959013\n",
      "Direct Solution SPO plus loss: 22.248348031103497\n",
      "Stochastic Gradient Descent Solution SPO plus loss: 32.695800144972864\n",
      "35\n",
      "Converged after 457 steps\n",
      "Direct Solution SPO loss: 6.7302039677134236\n",
      "Stochastic Gradient Descent Solution SPO loss: 9.373178266804317\n",
      "Direct Solution SPO plus loss: 26.278370438134033\n",
      "Stochastic Gradient Descent Solution SPO plus loss: 36.36889440998012\n",
      "36\n",
      "Converged after 466 steps\n",
      "Direct Solution SPO loss: 8.983431288531975\n",
      "Stochastic Gradient Descent Solution SPO loss: 11.352806574010865\n",
      "Direct Solution SPO plus loss: 27.46533060695072\n",
      "Stochastic Gradient Descent Solution SPO plus loss: 38.047549765367485\n",
      "37\n",
      "Converged after 524 steps\n",
      "Direct Solution SPO loss: 7.646683064121462\n",
      "Stochastic Gradient Descent Solution SPO loss: 9.477429119287407\n",
      "Direct Solution SPO plus loss: 21.656315261187178\n",
      "Stochastic Gradient Descent Solution SPO plus loss: 33.350856347717645\n",
      "38\n",
      "Converged after 339 steps\n",
      "Direct Solution SPO loss: 6.307700235374141\n",
      "Stochastic Gradient Descent Solution SPO loss: 9.978329154497988\n",
      "Direct Solution SPO plus loss: 23.827159670094474\n",
      "Stochastic Gradient Descent Solution SPO plus loss: 36.47655000151947\n",
      "39\n",
      "Converged after 698 steps\n",
      "Direct Solution SPO loss: 7.2234002555801995\n",
      "Stochastic Gradient Descent Solution SPO loss: 8.766642962655933\n",
      "Direct Solution SPO plus loss: 23.705853995234538\n",
      "Stochastic Gradient Descent Solution SPO plus loss: 30.8948507249702\n",
      "40\n",
      "Converged after 493 steps\n",
      "Direct Solution SPO loss: 7.538988399950358\n",
      "Stochastic Gradient Descent Solution SPO loss: 8.71495369728924\n",
      "Direct Solution SPO plus loss: 22.552725778325193\n",
      "Stochastic Gradient Descent Solution SPO plus loss: 31.142578480031005\n",
      "41\n",
      "Converged after 530 steps\n",
      "Direct Solution SPO loss: 9.345492391205845\n",
      "Stochastic Gradient Descent Solution SPO loss: 10.586858533472697\n",
      "Direct Solution SPO plus loss: 26.97103608045215\n",
      "Stochastic Gradient Descent Solution SPO plus loss: 34.820938002987425\n",
      "42\n",
      "Converged after 446 steps\n",
      "Direct Solution SPO loss: 6.817253213358454\n",
      "Stochastic Gradient Descent Solution SPO loss: 8.892060213119402\n",
      "Direct Solution SPO plus loss: 24.07193169745588\n",
      "Stochastic Gradient Descent Solution SPO plus loss: 34.828400790783896\n",
      "43\n",
      "Converged after 370 steps\n",
      "Direct Solution SPO loss: 8.853381004724943\n",
      "Stochastic Gradient Descent Solution SPO loss: 11.511588323846844\n",
      "Direct Solution SPO plus loss: 27.210645327561778\n",
      "Stochastic Gradient Descent Solution SPO plus loss: 34.960234129599264\n",
      "44\n",
      "Converged after 431 steps\n",
      "Direct Solution SPO loss: 7.771765649288755\n",
      "Stochastic Gradient Descent Solution SPO loss: 9.47780386918541\n",
      "Direct Solution SPO plus loss: 24.73783439061544\n",
      "Stochastic Gradient Descent Solution SPO plus loss: 36.27048162694792\n",
      "45\n",
      "Converged after 542 steps\n",
      "Direct Solution SPO loss: 8.416298008875238\n",
      "Stochastic Gradient Descent Solution SPO loss: 10.49929451615637\n",
      "Direct Solution SPO plus loss: 26.75849478087181\n",
      "Stochastic Gradient Descent Solution SPO plus loss: 35.245880605426876\n",
      "46\n",
      "Converged after 500 steps\n",
      "Direct Solution SPO loss: 10.029561480263292\n",
      "Stochastic Gradient Descent Solution SPO loss: 11.824510577371656\n",
      "Direct Solution SPO plus loss: 28.282666064999376\n",
      "Stochastic Gradient Descent Solution SPO plus loss: 37.19999258412945\n",
      "47\n",
      "Converged after 615 steps\n",
      "Direct Solution SPO loss: 7.554046520470474\n",
      "Stochastic Gradient Descent Solution SPO loss: 8.634223346884792\n",
      "Direct Solution SPO plus loss: 24.56240325235418\n",
      "Stochastic Gradient Descent Solution SPO plus loss: 32.57307504309331\n",
      "48\n",
      "Converged after 501 steps\n",
      "Direct Solution SPO loss: 8.554463432231568\n",
      "Stochastic Gradient Descent Solution SPO loss: 10.045530599441664\n",
      "Direct Solution SPO plus loss: 26.03167789863651\n",
      "Stochastic Gradient Descent Solution SPO plus loss: 33.28347971700949\n",
      "49\n",
      "Converged after 462 steps\n",
      "Direct Solution SPO loss: 7.697633970390998\n",
      "Stochastic Gradient Descent Solution SPO loss: 10.356211187212102\n",
      "Direct Solution SPO plus loss: 24.281954386003346\n",
      "Stochastic Gradient Descent Solution SPO plus loss: 34.829002909438024\n"
     ]
    }
   ],
   "source": [
<<<<<<< HEAD
    "B2 = DirectSolution(A, b, X, C,reg_weight=1)\n",
    "B3 = DirectSolution(A, b, X, C,reg_weight=10)\n",
    "print(f\"Regularization when 0: {np.linalg.norm(B1,'fro')}, 1:{np.linalg.norm(B2,'fro')}, 10:{np.linalg.norm(B3,'fro')}\")"
=======
    "for i in range(50):\n",
    "    print(i)\n",
    "    X, C = generate_data(n, p, grid_dim, sigma, noise, degree)\n",
    "    A,b = CreateShortestPathConstraints(grid_dim)\n",
    "    B_direct=DirectSolution(A,b, X, C)\n",
    "    B_SGD=GradientDescentSolution(A,b, X, C, batch_size=10,epsilon = 0.001) \n",
    "\n",
    "    solver = ShortestPathSolver(A,b)\n",
    "    loss_direct = SPOLoss(solver, X, C, B_direct)\n",
    "    loss_SGD = SPOLoss(solver, X, C, B_SGD)\n",
    "    print(f\"Direct Solution SPO loss: {loss_direct}\")\n",
    "    print(f\"Stochastic Gradient Descent Solution SPO loss: {loss_SGD}\")\n",
    "\n",
    "\n",
    "    loss_plus_direct = SPOplusLoss(solver, X, C, B_direct)\n",
    "    loss_plus_SGD = SPOplusLoss(solver, X, C, B_SGD)\n",
    "    print(f\"Direct Solution SPO plus loss: {loss_plus_direct}\")\n",
    "    print(f\"Stochastic Gradient Descent Solution SPO plus loss: {loss_plus_SGD}\")\n",
    "    \n",
    "    if loss_plus_direct>loss_plus_SGD:\n",
    "        break\n",
    "    if loss_plus_direct<0:\n",
    "        break\n",
    "    if loss_plus_SGD<0:\n",
    "        break"
>>>>>>> 180c3a30d2471a9f71164389427bdefe87dda679
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment Generation \n",
    "\n",
    "'''\n",
    "######## from data generation helper function #########\n",
    "    Generate data for nxn grid\n",
    "\n",
    "    Parameters:\n",
    "        int n: number of data points to generate\n",
    "        int p: number of features\n",
    "        int grid_dim: Dimension of square grid, determines size of cost vector\n",
    "        array sigma: array of length p, is the variance of each feature vector dimension, i.e. x_i ~ N(0, sigma_p)\n",
    "        float noise: multiplicative noise term applied to cost vector, sampled from uniform distribution in [1-noise, 1+noise]\n",
    "        int degree: polynomial degree of generated cost vector. When degree=1, expected value of c is linear in x. Degree > 1               controls the amount of model misspecification.\n",
    "\n",
    "    Returns:\n",
    "        np.array X: feature data of dimension [num_samples, p]\n",
    "        np.array C: cost data of dimension [num_samples, d]\n",
    "    '''\n",
    "'''\n",
    "Generate data for different instances of n and p \n",
    "\n",
    "'''\n",
    "def generate_sigma(p):\n",
    "    ''' \n",
    "    Generates a list 'sigma' of length p, where sigma is the variance of each feature vector dimension i.e. x_i ~ N(0, sigma_p)\n",
    "\n",
    "    input: p (int) represents the number of features\n",
    "\n",
    "    returns: sigma (list of floats)\n",
    "    ''' \n",
    "    sigma = []\n",
    "    for i in range(p): \n",
    "        num = np.random.normal()\n",
    "\n",
    "    return sigma \n",
    "n = 100\n",
    "p = 5 \n",
    "grid_dim = 5 \n",
    "sigma = [0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "noise = 0.25\n",
    "degree = 3\n",
    "\n",
    "X, C = generate_data(n, p, grid_dim, sigma, noise, degree)\n",
    "A,b = CreateShortestPathConstraints(grid_dim)\n",
    "B1 = DirectSolution(A, b, X, C)\n",
    "\n"
   ]
=======
   "execution_count": null,
   "id": "clinical-cambodia",
   "metadata": {},
   "outputs": [],
   "source": []
>>>>>>> 180c3a30d2471a9f71164389427bdefe87dda679
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}